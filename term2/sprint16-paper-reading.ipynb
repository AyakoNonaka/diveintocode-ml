{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint16課題 論文読解入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題】Faster R-CNN[8]を読んで以下に答える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstraact\n",
    "\n",
    "-SPPnet\n",
    "-Fast R-CNN\n",
    "-RPN\n",
    "-R-CNNs\n",
    "    \n",
    "Introduction\n",
    "\n",
    "-Selective Search\n",
    "-EdgeBoxes\n",
    "    \n",
    "Related Work \n",
    "\n",
    "-CPMC\n",
    "-MCG\n",
    "\n",
    "EXPERIMENTS\n",
    "\n",
    "-VGG16\n",
    "-Zf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "\n",
    "-In previous research, object detection networks have been reduced the running time of the detection networks.\n",
    "Althogh, exposing region proposal computation is still a bottleneck to hypothesize object locations.\n",
    "\n",
    "-The authors merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Experiments on PASCAL VOC\n",
    "\n",
    "-One-Stage Detection is a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. \n",
    "\n",
    "-One-Stage is a class-specific detection pipeline, and Two-Stage is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. \n",
    "\n",
    "-Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN—the downstream Fast R-CNN detector attends to the proposals to refine them. \n",
    "\n",
    "-In the second stage of the Two-Stage, the region-wise features are adaptively pooled from proposal boxes that more faithfully cover the features of the regions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstraact\n",
    "\n",
    "-A Region Proposal Network (RPN)  is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.\n",
    "\n",
    "-The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. \n",
    "\n",
    "3.1 Region Proposal Networks\n",
    "\n",
    "-An RPN takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.RoIプーリングとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference_[2] R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.\n",
    "\n",
    "-The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W, where H and W are layer hyper-parameters that are independent of any particular RoI. \n",
    "\n",
    "-RoI stands for regions of interest.\n",
    "\n",
    "-Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). See diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RoIpooling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Anchorのサイズはどうするのが適切か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1 Anchors\n",
    "\n",
    "-At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k.So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal. \n",
    "The k proposals are parameterized relative to k reference boxes, which we call anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W Hk anchors in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ancor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "\n",
    "-For the very deep VGG-16 model, this detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image.\n",
    "\n",
    "\n",
    "4 EXPERIMENTS\n",
    "\n",
    "-Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.（オプション）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
