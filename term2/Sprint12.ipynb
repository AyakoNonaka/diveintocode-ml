{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        X = X - np.max(X)\n",
    "        tmp = np.exp(X)\n",
    "        denominator = np.sum(tmp, axis=1)\n",
    "        output = tmp / denominator[:, np.newaxis]\n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( x > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, optimizer):\n",
    "        DIM = 1\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = len(f_w)\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w[:, np.newaxis]\n",
    "        self.B = f_b[:, np.newaxis]\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dB = 0\n",
    "        print(\"N_input:{} F_hight:{}\".format(self.n_input_hight, self.f_hight))\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        #self.n_output_width = self.n_input_width - f_width +1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros([self.n_output_hight, DIM])\n",
    "        self.W_feedback = np.zeros([self.f_hight, DIM])\n",
    "        self.B_feedback = 0\n",
    "        self.Z_feedback = np.zeros([self.n_input_hight, DIM])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            \n",
    "            X_seg = X[h1:h2]\n",
    "            self.output_X_forward[h] = np.dot(X_seg, self.W) + self.B\n",
    "        \n",
    "        return self.output_X_forward\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        dA = dA[:,np.newaxis]\n",
    "        for i in range(self.f_hight):\n",
    "            X_seg = self.input_X_forward[i : (i + self.n_output_hight)]\n",
    "            X_seg = X_seg[:,np.newaxis]\n",
    "            self.W_feedback[i] = np.dot(X_seg.T, dA)\n",
    "        \n",
    "        #Bについて\n",
    "        self.B_feedback = np.sum(dA, axis=0)\n",
    "        \n",
    "        #Zについて\n",
    "        #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "        dA_padding = np.zeros([self.f_hight-1, 1])\n",
    "        dA = np.concatenate((dA, dA_padding), axis=0)\n",
    "        dA = np.concatenate((dA_padding, dA), axis=0)\n",
    "        for h in range(self.n_input_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            dA_seg = dA[h1:h2]\n",
    "            #並列計算工夫\n",
    "            dA_seg = np.fliplr(dA_seg.T).T\n",
    "            self.Z_feedback[h] = np.dot(dA_seg.T, self.W)\n",
    "            #print(\"h:{} \\n dA_seg:{} \\n W:{}\".format(h, dA_seg, self.W))\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_output_size(n_feature_in, n_pading, n_filter, stride):\n",
    "    return (n_feature_in + n_pading * 2 + n_filter) / stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initializer = SimpleInitializer()\n",
    "optimizer = SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_input:4 F_hight:3\n"
     ]
    }
   ],
   "source": [
    "scv = SimpleConv1d(len(x), w, b, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.],\n",
       "       [50.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.5],\n",
       "       [108.2],\n",
       "       [167.3],\n",
       "       [137.8]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b: [30]\n",
      "delta_w: [[ 50.]\n",
      " [ 80.]\n",
      " [110.]]\n",
      "delta_x: [[ 29.5]\n",
      " [108.2]\n",
      " [167.3]\n",
      " [137.8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"delta_b:\",scv.B_feedback)\n",
    "print(\"delta_w:\",scv.W_feedback)\n",
    "print(\"delta_x:\",scv.Z_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "紙やホワイトボードを使い計算グラフを書きながら考えてください。\n",
    "例えば以下のようなx, w, bがあった場合は"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。  \n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。  \n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）  \n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        #for output_ch in range(self.W.shape[0]):\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        #self.input_X_forward = np.zeros((self.n_output_hight, X.shape[0], self.f_hight))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        #Row = 入力数＊出力数, Col = 出力の特徴量数　の入力X,誤差dLを作る \n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "        \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "        #Bについて\n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4],[2,3,4,5]])\n",
    "W = np.ones((3,2,3))\n",
    "B = np.array([[[1,2,3], [1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Conv1d(X.shape[1], W, B, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dA = np.array([[2,4],[4,8],[6,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 36, 36, 24],\n",
       "       [12, 36, 36, 24]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題5】学習・推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えて学習と推定を行ってください。出力層だけは全結合層をそのまま使ってください。\n",
    "チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、平滑化を行います。平滑化はNumPyのreshapeが使用できます。\n",
    "numpy.reshape — NumPy v1.15 Manual\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                \n",
    "                #Loss計算\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.inout_X_shape = X.shape\n",
    "        return X.reshape(-1)[np.newaxis,:]\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784)\n",
      "(57000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN = ScratchDeepNeuralNetrowkClassifier(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CNN Filter(重み、バイアス)\n",
    "f_w = np.ones((3,1,28))\n",
    "f_b = np.array([[[1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#初期化、更新インスタンスを作る\n",
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN.add_layer(Conv1d(x_train.shape[1], f_w, f_b, initializer, optimizer))\n",
    "CNN.add_layer(Flatten())\n",
    "CNN.add_layer(FC(f_w.shape[0] * (x_train.shape[1] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN.add_layer(Sigmoid())\n",
    "CNN.add_layer(FC(100, 10, initializer, optimizer))\n",
    "CNN.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,10) (3,1,28) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-4ec66ffec15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-d41709bc585f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;31m#X = self.affine_func[layer].forward(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m#X = self.activation_func[layer].forward(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m#Loss計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-38815ef9d2fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_hight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mX_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,10) (3,1,28) "
     ]
    }
   ],
   "source": [
    "CNN.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-bcb33870130b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-d41709bc585f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m#X = self.affine_func[layer].forward(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m#X = self.activation_func[layer].forward(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-38815ef9d2fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_hight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mX_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred = CNN.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-aadc7aabf2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pred=\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Yval=\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-20d978b20d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy score={:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred[0], y_val[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題6】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングを加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "最も単純なパディングは全て0で埋めるゼロパディングであり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。\n",
    "なお、NumPyにはパディングの関数が存在します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, padding_size, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.padding_size = padding_size\n",
    "        self.n_input_hight = n_input_hight + self.padding_size * 2\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        #Padding\n",
    "        X = np.pad(X, (self.padding_size, self.padding_size), 'constant', constant_values=(0, 0))\n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        #Row = 入力数＊出力数, Col = 出力の特徴量数　の入力X,誤差dLを作る \n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "        \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "        #Bについて\n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題7】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 784)\n",
      "(57000, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "#0~255を0~1スケールへ\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#0~9をone-hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "#chを追加\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_mini_batch = GetMiniBatch(x_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_X_train, mini_y_train = get_mini_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 784)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_input, n_feature1)\n",
    "        \n",
    "        return (batch_size, n_input * n_feature)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[[1,2],[2,3]],[[4,5],[6,7]],[[8,9],[10,11]]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                    #print(\"layer:{} X.shape:{}\".format(layer, X.shape))\n",
    "                \n",
    "                #Loss計算\n",
    "                #print(\"X.shape:{} Y.shape:{}\".format(X.shape, mini_y_train.shape))\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                    #print(\"layer:{} dz.shape:{}\".format(layer, dz.shape))\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC2:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        #print(\"self X_forward:\",self.input_X_forward.shape)\n",
    "        #print(\"dA:\",dA.shape)\n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        #print(\"dW:\",dW.shape)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        #print(\"W.shape:\",self.W.shape)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight) = > (batch, 1, ch, hight)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            X_seg = X[:,:,:,h1:h2]\n",
    "            \n",
    "            #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "            #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "            tmp = np.sum(X_seg * self.W, axis=3)\n",
    "            #print(\"tmp1.shape:\",tmp.shape)\n",
    "            #print(\"B.shape:\",B.shape)\n",
    "            tmp = tmp + B\n",
    "            A[:,:,:,h] = tmp\n",
    "\n",
    "        A = np.sum(A, axis=2)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature1)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((dA.shape[0], X.shape[1], dA.shape[2]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.n_input_hight - self.n_output_hight + 1\n",
    "        dW_tmp = np.zeros((batch_size, self.n_output, loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + self.n_output_hight\n",
    "            dX_seg = X[:,:, i1:i2]\n",
    "            dW_tmp[:,:,i] = np.sum(dL * dX_seg, axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(dW_tmp2.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature2)\n",
    "        dB = np.sum(dA, axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1])\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            #print(\"dA_tmp.shape1:\",dA_tmp.shape)\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2)\n",
    "            #print(\"dA_tmp.shape2:\",dA_tmp.shape)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,:,h] = np.sum(dA_seg * self.W[i], axis=2)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CNN Filter(重み、バイアス)\n",
    "f_w = np.ones((1,1,28))\n",
    "f_b = np.array([[[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = CNN2.forward(mini_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 757)\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN2.backward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN2.add_layer(Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten())\n",
    "CNN2.add_layer(FC2(f_w.shape[0] * (x_train.shape[2] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Yval=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4lGXWx/HvIYQmoCgdpCgoCkjAUBRErPBawIIQKYqKrLpiWUV0rYu9rK4F14rgioBiWQUlWFYBBSVAAFHEigREinSkJff7xz0pYBrJzDwzye9zXblIZp55njOjM2fudm5zziEiIgJQIegAREQkdigpiIhIDiUFERHJoaQgIiI5lBRERCSHkoKIiORQUhAphJk1MzNnZhWjeM0eZpYRreuJ5KWkICIiOZQUREQkh5KCxBUza2hmb5jZWjP7ycyuyXPfXWY22cwmmdkWM5tvZu3y3H+UmX1iZhvNbImZ9c5zX1Uz+6eZLTezTWY2y8yq5rn0QDP7xczWmdmtBcTWxcxWm1lCntvONbNFod87mVmamW02s9/M7NFiPufC4j7DzL4OPd+VZnZj6PbaZjYl9JjfzWymmen9LkXS/yQSN0Ifau8CC4FGwCnAdWbWM89hfYDXgYOBV4G3zSzRzBJDj50O1AWGA+PN7MjQ4x4BjgWODz32JiArz3m7AUeGrnmHmR21b3zOuTnANuDkPDcPCMUB8DjwuHOuJnA48FoxnnNRcb8I/MU5VwNoA3wcuv0GIAOoA9QD/g6opo0USUlB4klHoI5zbpRzbpdz7kfgeSAlzzHznHOTnXO7gUeBKkCX0E914IHQYz8GpgAXhpLNpcC1zrmVzrlM59znzrmdec77D+fcH865hfik1I78TQAuBDCzGsAZodsAdgMtzKy2c25rKIkUpcC485zzaDOr6Zzb4Jybn+f2BkBT59xu59xMp0JnUgxKChJPmgINQ10iG81sI/4bcL08x6zI/sU5l4X/ttww9LMidFu25fgWR2188vihkGuvzvP7dvwHdX5eBc4zs8rAecB859zy0H2XAUcAS81srpmdVeiz9QqLG+B8fOJZbmafmtlxodsfBr4HppvZj2Z2czGuJaKkIHFlBfCTc+6gPD81nHNn5Dnm0OxfQi2AxsCq0M+h+/SrNwFWAuuAHfgunVJxzn2N/9D+P/buOsI5951z7kJ8N9CDwGQzO6CIUxYWN865uc65PqFzvk2oS8o5t8U5d4Nz7jDgbOBvZnZKaZ+flH1KChJPvgQ2m9nI0MBwgpm1MbOOeY451szOC60ruA7YCcwBvsD3998UGmPogf+wnBj6Fj4GeDQ0kJ1gZseFvu2XxKvANUB3/PgGAGY2yMzqhK63MXRzZhHnKjBuM6tkZgPN7MBQd9nm7POZ2Vlm1sLMLM/tRV1LRElB4odzLhP/gZgE/IT/hv8CcGCew/4L9Ac2AIOB80J96ruA3vhv8OuAp4GLnHNLQ4+7EVgMzAV+x3+TL+n7YwLQA/jYObcuz+29gCVmthU/6JzinNtRxHMuKu7BwM9mthm4AhgUur0l8CGwFZgNPO2c+6SEz0fKEdPYk5QVZnYX0MI5N6ioY0Ukf2opiIhIDiUFERHJoe4jERHJoZaCiIjkiFo54HCpXbu2a9asWdBhiIjElXnz5q1zztUp6ri4SwrNmjUjLS0t6DBEROKKmS0v+ih1H4mISB5KCiIikkNJQUREcsTdmIKIlE+7d+8mIyODHTsKrQxS7lWpUoXGjRuTmJhYoscrKYhIXMjIyKBGjRo0a9YMX+dP9uWcY/369WRkZNC8efMSnUPdRyISF3bs2MEhhxyihFAIM+OQQw4pVWtKSUFE4oYSQtFK+xqVn6SwciVcdx3s3h10JCIiMav8JIUvv4THH4dRo4KORETiVPXqBe3CWnaUn6Rw7rlw8cVw330we3bQ0YiIxKTykxQAnngCDj0UBg+GrVuDjkZE4pRzjhEjRtCmTRvatm3LpEmTAPj111/p3r07SUlJtGnThpkzZ5KZmcmQIUNyjn3ssccCjr5w5WtKas2a8PLL0KMH3HADPPts0BGJSElcdx2kp4f3nElJ8K9/FevQN998k/T0dBYuXMi6devo2LEj3bt359VXX6Vnz57ceuutZGZmsn37dtLT01m5ciVfffUVABs3bizi7MEqXy0FgO7dYcQIeO45ePfdoKMRkTg0a9YsLrzwQhISEqhXrx4nnngic+fOpWPHjrz00kvcddddLF68mBo1anDYYYfx448/Mnz4cKZNm0bNmjWDDr9Q5aulkG3UKEhNhaFDYfFiqFs36IhEZH8U8xt9pBS0OVn37t2ZMWMGU6dOZfDgwYwYMYKLLrqIhQsXkpqayujRo3nttdcYM2ZMlCMuvvLXUgCoXBleeQU2boRhw0C7z4nIfujevTuTJk0iMzOTtWvXMmPGDDp16sTy5cupW7cul19+OZdddhnz589n3bp1ZGVlcf7553P33Xczf/78oMMvVPlsKQC0aQP33+/HFl56CS69NOiIRCROnHvuucyePZt27dphZjz00EPUr1+fcePG8fDDD5OYmEj16tV5+eWXWblyJZdccglZWVkA3H///QFHX7i426M5OTnZhW2TnawsOPVUmDsXFi6Eww4Lz3lFJOy++eYbjjrqqKDDiAv5vVZmNs85l1zUY8tn91G2ChVg7FhISPDTVDMzg45IRCRQ5TspADRpAqNHw+efw0MPBR2NiEiglBQABgyAfv3gjjtgwYKgoxERCUzEkoKZjTGzNWb2VSHH9DCzdDNbYmafRiqWIpnBv//tp6YOHAh//BFYKCIiQYpkS2Es0KugO83sIOBpoLdzrjVwQQRjKdrBB/vxhW++gVtuCTQUEZGgRCwpOOdmAL8XcsgA4E3n3C+h49dEKpZiO+00GD7cV1P98MOgoxERibogxxSOAGqZ2SdmNs/MLiroQDMbZmZpZpa2du3ayEb1wAPQqhUMGQIbNkT2WiIiMSbIpFAROBY4E+gJ3G5mR+R3oHPuOedcsnMuuU6dOpGNqlo1v9r5t9/gqqsiey0RKbMK23vh559/pk2bNlGMpviCTAoZwDTn3Dbn3DpgBtAuwHhyHXss3HUXTJwIEyYEHY2ISNQEWebiv8BTZlYRqAR0BmKn0PjIkTB1qm8tdOvm92EQkZgQROXskSNH0rRpU64K9SDcddddmBkzZsxgw4YN7N69m3vuuYc+ffrs13V37NjBlVdeSVpaGhUrVuTRRx/lpJNOYsmSJVxyySXs2rWLrKws3njjDRo2bEi/fv3IyMggMzOT22+/nf79+5fmaf9JxJKCmU0AegC1zSwDuBNIBHDOPeOc+8bMpgGLgCzgBedcgdNXo65iRb/3QlISXHIJTJ/uV0CLSLmUkpLCddddl5MUXnvtNaZNm8b1119PzZo1WbduHV26dKF3796YWbHPO3r0aAAWL17M0qVLOf3001m2bBnPPPMM1157LQMHDmTXrl1kZmby3nvv0bBhQ6ZOnQrApk2bwv48I5YUnHMXFuOYh4GHIxVDqbVoAY895iupPvGE/3oiIoELonJ2+/btWbNmDatWrWLt2rXUqlWLBg0acP311zNjxgwqVKjAypUr+e2336hfv36xzztr1iyGDx8OQKtWrWjatCnLli3juOOO49577yUjI4PzzjuPli1b0rZtW2688UZGjhzJWWedxQknnBD256mvvkUZOhTOPhtuvhmWLAk6GhEJUN++fZk8eTKTJk0iJSWF8ePHs3btWubNm0d6ejr16tVjx44d+3XOgoqSDhgwgHfeeYeqVavSs2dPPv74Y4444gjmzZtH27ZtueWWWxg1alQ4ntZelBSKYgbPP++38hw0CHbtCjoiEQlISkoKEydOZPLkyfTt25dNmzZRt25dEhMT+d///sfy5cv3+5zdu3dn/PjxACxbtoxffvmFI488kh9//JHDDjuMa665ht69e7No0SJWrVpFtWrVGDRoEDfeeGNE9mZQUiiOevXghRf8yNZddwUdjYgEpHXr1mzZsoVGjRrRoEEDBg4cSFpaGsnJyYwfP55WrVrt9zmvuuoqMjMzadu2Lf3792fs2LFUrlyZSZMm0aZNG5KSkli6dCkXXXQRixcvplOnTiQlJXHvvfdy2223hf05lu/9FPbX5ZfDmDHw6ad+RpKIRI32Uyg+7acQLY8+Cs2a+b0XNm8OOhoRkbArv9txlkSNGn6aavfucP318OKLQUckIjFs8eLFDB48eK/bKleuzBdffBFQREVTUthfXbv6mUj33ednJZ1zTtARiZQbzrn9WgMQtLZt25Ie7lV2RSjtkIC6j0rizjuhQwc/xrB6ddDRiJQLVapUYf369aX+0CvLnHOsX7+eKlWqlPgcaimURKVK8J//+BpJQ4fCu+/6qasiEjGNGzcmIyODiFdKjnNVqlShcePGJX68kkJJHX00PPggXHutX8cwbFjQEYmUaYmJiTRv3jzoMMo8dR+VxtVXw6mn+kHn774LOhoRkVJTUiiNChX8Fp6VK/tpqnv2BB2RiEipKCmUVqNG8O9/wxdfwP33Bx2NiEipKCmEQ//+MGAA/OMfMHdu0NGIiJSYkkK4jB4NDRr4bqTt24OORkSkRJQUwuWgg2DcOPj2W7jppqCjEREpkYglBTMbY2ZrzCzf3dTMrIeZbTKz9NDPHZGKJWpOPtnPRBo9GqZNCzoaEZH9FsmWwligVxHHzHTOJYV+wr9bRBDuuw9at4ZLL4X164OORkRkv0QsKTjnZgC/R+r8MatKFXjlFVi3Dq64ArQkX0TiSNBjCseZ2UIze9/MWhd0kJkNM7M0M0uLiyXuSUlw990webJPECIicSKim+yYWTNginOuTT731QSynHNbzewM4HHnXMuizhnoJjv7IzMTevSARYv8T9OmQUckIuVYzG+y45zb7JzbGvr9PSDRzGoHFU/YJST4vRecg4svhqysoCMSESlSYEnBzOpbqDC6mXUKxVK2RmabN4cnnvDbdz76aNDRiIgUKWJVUs1sAtADqG1mGcCdQCKAc+4ZoC9wpZntAf4AUlxZLJR+8cXwzjtw661w+ulwzDFBRyQiUqCIjilEQtyMKeS1di20bQt16/oyGJUrBx2RiJQzMT+mUK7UqQNjxsDixXD77UFHIyJSICWFaDnjDL9u4ZFH/BiDiEgMUlKIpkcegcMPh4sugk2bgo5GRORPlBSi6YAD/GK2lSvhmmuCjkZE5E+UFKKtc2e47Ta/hmHy5KCjERHZi5JCEG69FTp2hL/8BVatCjoaEZEcSgpBSEyE//wH/vjDV1ONs2nBIlJ2KSkE5cgj/cBzaqrf41lEJAYoKQTpyiuhVy+48Ua/Y5uISMCUFIJk5he1Va0KgwbB7t1BRyQi5ZySQtAaNIDnnoO0NLjnnqCjEZFyTkkhFpx/vi+cd++9MGdO0NGISDmmpBArHn8cGjeGwYNh69agoxGRckpJIVYceCCMGwc//OAHnkVEAqCkEEtOPNEnhGefhalTg45GRMohJYVYc/fdfiOeyy7z+zCIiERRxJKCmY0xszVm9lURx3U0s0wz6xupWOJK5cq+aN6GDTBsmFY7i0hURbKlMBboVdgBZpYAPAikRjCO+NO2Ldx3H7z9NowdG3Q0IlKORCwpOOdmAL8Xcdhw4A1gTaTiiFvXXw89evgS2z/9FHQ0IlJOBDamYGaNgHOBZ4px7DAzSzOztLXlpZ+9QgU/G6lCBb8pT2Zm0BGJSDkQ5EDzv4CRzrkiP+2cc88555Kdc8l16tSJQmgxokkTGD0aZs2Chx8OOhoRKQeCTArJwEQz+xnoCzxtZudE8oI7dkTy7BEycCBccAHccQcsWBB0NCJSxgWWFJxzzZ1zzZxzzYDJwFXOubcjdb1PPvHbI0+fHqkrRIgZPPMM1K7ti+bFZWYTkXgRySmpE4DZwJFmlmFml5nZFWZ2RaSuWZg6daBWLejZE0aMgF27goiihA4+GF56Cb7+Gv7+96CjEZEyzFyczYNPTk52aWlpJXrsH3/ADTf4PW2Sk+HVV6FlyzAHGEnDh8NTT8GHH8IppwQdjYjEETOb55xLLuq4crWiuWpVePppePNNX2KoQwe/K2bcePBBaNUKhgzxi9tERMKsXCWFbOeeCwsX+qRw0UW+MOnmzUFHVQzVqvkstno1XH110NGISBlULpMCwKGHwscfw6hRvhupQweYOzfoqIohORnuvNMHPXFi0NGISBlTbpMCQEIC3H47fPqp3wnz+OP9coCsrKAjK8LNN0OXLn6P54yMoKMRkTKkXCeFbN26QXo6nHMO3HQT9Orle2hiVsWKvhtp924/vhDzWUxE4oWSQkitWvDaa3675FmzfPXq998POqpCtGgBjz0GH33kZySJiISBkkIeZnD55ZCWBvXrwxlnwN/+Bjt3Bh1ZAYYOhbPOgpEj/RoGEZFSUlLIx9FHw5df+gk+jz0Gxx0Hy5YFHVU+zOCFF6BGDb/aOa5W5IlILFJSKECVKvDkk/Df/8Ivv/jZSWPHxuCeN/XqwfPP+7pI//hH0NGISJxTUihC795+TUPHjnDJJb4+3aZNQUe1jz59/PadDzwAn30WdDQiEseUFIqhUSNfWeLee/1gdPv2MGdO0FHt47HHoGlTvxpvy5agoxGROKWkUEwJCb4W3cyZfgZot25w//0xNBu0Rg0/TfXnn/2ubSIiJaCksJ+OO86vaejb1yeJ006DVauCjiqka1e/sO3FF/1giIjIflJSKIGDDoIJE/xn75w5fk3DlClBRxVy552+f+vyy+G334KORkTijJJCCZnBpZfCvHm+jtLZZ8O118bAHjiVKsErr/gKf0OHxuB0KRGJZUoKpdSqlW8tXHcdPPGEL0m0dGnAQR19tC+zPWWKX8cgIlJMkdx5bYyZrTGzrwq4v4+ZLTKzdDNLM7NukYol0ipX9pN/pkyBlSvh2GN911KgX9KHD4dTT/WDzt9/H2AgIhJPItlSGAv0KuT+j4B2zrkk4FIg7r/SnnkmLFrkB6OHDoX+/WHjxoCCqVDBb+GZmOinqe7ZE1AgIhJPIpYUnHMzgN8LuX+ry90L9ACgTHR+N2gA06f7dWRvvQVJSfD55wEF07ix33t09mwfkIhIEQIdUzCzc81sKTAV31oo6LhhoS6mtLVr10YvwBKqUMHXqJs1y//evTvccw9kZgYQTEoKDBjgS2CUcG9rESk/zEWw49vMmgFTnHNtijiuO3CHc+7Uos6ZnJzs0uLow23zZrjiCj+F9cQT/cSgxo2jHMSGDX7ebPXqfrpUtWpRDkBEgmZm85xzyUUdFxOzj0JdTYebWe2gYwm3mjVh/HhfTC8tDdq1C2BdWa1aPoClS/3iNhGRAhQrKZjZtWZW07wXzWy+mZ1emgubWQszs9DvHYBKwPrSnDNWmcHFF8P8+dCsmd/h7eqr4Y8/ohjEKaf4ebNPPukHPURE8lHclsKlzrnNwOlAHeASoNCRSzObAMwGjjSzDDO7zMyuMLMrQoecD3xlZunAaKC/i2RfVgw44gg/6HzDDTB6NHTuHOW9ce6/369hGDIE1pfJ/CsipVSsMQUzW+ScO8bMHgc+cc69ZWYLnHPtIx/i3uJtTKEg06b51sOWLfCvf/mqFL7dFGELFvhsdM45MGlSlC4qIkEL95jCPDObDpwBpJpZDSBW6oPGpV69/D4NJ5wAf/mLL7D3e4ETeMOofXsYNQpefx1efTUKFxSReFLcpHAZcDPQ0Tm3HUjEdyFJKdSvD++/Dw8/DO+849c0zJwZhQuPGOFrf//1r35bORGRkOImheOAb51zG81sEHAbEGv7j8WlChXgxhv9WEOlStCjh19SENEFyAkJ8PLLfuHExRfH0KYQIhK04iaFfwPbzawdcBOwHHg5YlGVQx07+u7+QYPgrrvg5JNhxYoIXrB5c1/B75NP/KCGiAjFTwp7QjOD+gCPO+ceB2pELqzyqUYNGDfOb6C2YIFf0/DmmxG84JAhfsD5llvgq3zrFopIOVPcpLDFzG4BBgNTzSwBP64gETBokE8KLVrA+ef7FdHbt0fgQmbw3HN+cdvAgbBzZwQuIiLxpLhJoT+wE79eYTXQCHg4YlEJLVr42kk33QTPPgudOkXoy3ydOr7O96JFcMcdEbiAiMSTYiWFUCIYDxxoZmcBO5xzGlOIsEqV/F45qamwbp0fd3j66Qjs03DmmX5e7MMPw4wZYT65iMST4pa56Ad8CVwA9AO+MLO+kQxMcp1+uv8i36OHn0V67rkRWJD8yCNw+OF+74VNmlgmUl4Vt/voVvwahYudcxcBnYDbIxeW7KtuXZg6FR59FN57zw9Cf/ppGC9Qvbov4ZqR4TebFpFyqbhJoYJzbk2ev9fvx2MlTCpU8Ltrzpnjq1+fdJIfBgjbmobOneHWW/0UqDfeCNNJRSSeFPeDfZqZpZrZEDMbgt8U573IhSWF6dDBV1wdMgTuvtvv07B8eZhOftttkJzsxxh+/TVMJxWReFHcgeYRwHPAMUA74Dnn3MhIBiaFq14dxozx5YsWL/bdSa+/HoYTJyb6bqTt2+HSS7XaWaScKXYXkHPuDefc35xz1zvn3opkUFJ8F14I6enQqhX06wfDhsG2baU86ZFH+oHnadOga1f48suwxCoisa/QpGBmW8xscz4/W8xsc7SClMIddpgvpHfLLfDCC773Z+HCUp70yivhpZfgp5/8WMOQIepOEikHCk0Kzrkazrma+fzUcM7VjFaQUrTERLjvPvjgAz+jtHNnv8laidc0mPlEsGwZjBzpN5k+4gi/Uc+OHeEMXURiSMRmEJnZGDNbY2b5rsM1s4Fmtij083mo2J6U0imn+FbCqafCNddAnz5+4VuJ1awJDzzgt4g79VT4+9/97m1vvRWBVXQiErRITisdC/Qq5P6fgBOdc8cAd+MHsiUM6tSBd9/1RVBTU+GYY+Djj0t50sMP94nggw/8fNjzzvNJYvHisMQsIrEhYknBOTcDKHAvMefc5865DaE/5wCNIxVLeWQGw4fDF1/AgQf6z+9bb4Xdu0t54lNP9SPbo0f7f5OS4KqrStkcEZFYESsL0C4D3i/oTjMbZmZpZpa2du3aKIYV/5KSIC0NLrvMjzl07+7HjkulYkWfCL77ztfdeO45aNnSN01KnXVEJEiBJwUzOwmfFApc9+Cce845l+ycS65Tp070gisjDjgAnn8eJk2Cb77xiWLixDCc+OCDfSJYuNBX67v2Wr9gIjU1DCcXkSAEmhTM7BjgBaCPcy7cJd5kH/36+R6f1q39+oZLLw3DmgbwJ0xN9RtN794NvXrB2Wf7mUsiElcCSwpm1gR4ExjsnNOnR5Q0a+arY992G4wd60tmLFgQhhOb+UTw1Vfw0EO+Wl+bNn4DalVdFYkbkZySOgGYDRxpZhlmdpmZXWFmV4QOuQM4BHjazNLNLC1SscjeKlb0NZM+/hi2boUuXfw2zWGZYVq5MowY4ccbLrrIl3Vt2dKvqsvMDMMFRCSSzMXZXPPk5GSXlqb8ES7r1vlB6HfegTPO8IuY69YN4wXmz/djDbNm+cGMxx/3o90iElVmNs85l1zUcYEPNEuwateGt9/2M0w/+siPE3/4YRgv0KGD76+aONHvDHTiidC/fxjLuopIOCkpCGZ+huncuX5C0emnw803h3F2qZlPBEuXwl13+ZV1rVr5zSDCMtItIuGipCA52rb1iWHYML83dNeuuROKwqJaNbjzTvj2W7+n6N13+4qsr76qkhkiMUJJQfZSrRo884zfeG3FCl87qXFjuOEGP7EoLA491CeCmTOhfn0YOBC6dfMZSUQCpaQg+TrvPPjlF99S6NbNV1xt29aX5X7qKT88UGrduvm9GsaMgR9+gE6d4JJLVKJbJEBKClKgxES/9OCNN2DVKj9xKCvL11Rq2BAuuACmTi3lHtEVKvhEsGwZ3HQTjB/vS3Q/+CDs3Bm25yIixaOkIMVSu7YvxT1/vl8VfdVV8MkncNZZvjfoppt8de0Sq1nTJ4Kvv/b1v2++2ZfofvttjTeIRJGSguy3du3gscdg5UpfTbtzZ/9369b+93//GzZsKPo8+WrRwieC6dOhShU/IH3aaWEc0BCRwigpSIlVqgTnnOM/w1eu9IuX//jDtyIaNICUFL/Nc4kWMp92mi+09+STvnnSrp2vyBqWwQwRKYiSgoRF3bpw/fX+c3z+fPjLX/wiuP/7P2jSxO8fvXTpfp60YkW4+mpfMuOqq+DZZ33JjCefVIlukQhRUpCwMoP27f2g9MqVfpC6Qwd4+GE46ig47ji//cJ+1cg75BCfCNLT4dhj/eBGUpLvYhKRsFJSkIipXNlPbX33XcjIgEcegS1bfCuifn0YMMDv7lns7qU2bXwi+O9//cyknj2hd2/fkhCRsFBSkKioX98vgFu82K9Ru+wyP95w+um+nPdttxXzs93MJ4IlS/xspf/9z49wjxihEt0iYaCkIFFllrsAbtUqeO01OOYYuP9+vzyhWzd48UXYvLmIE1Wu7OfBfvcdDB4M//ynP8GLL6pEt0gpKClIYKpUyV0At2KF/+K/fj0MHepbFoMH+z0fsrIKOUn9+j4RfPmln846dKhfGT1rVtSeh0hZoqQgMaFhw9wFcHPmwMUX+7GIU06B5s19QdUffijkBMnJPhFMmABr1sAJJ/g5sb/8ErXnIFIWRHLntTFmtsbM8l11ZGatzGy2me00sxsjFYfEF7PcBXC//uo/4486Cu65xzcETjzRbwS0dWsBD05J8VVY77zTD0gfeaT/XSW6RYolki2FsUCvQu7/HbgGeCSCMUgcq1o1dwHcL7/AfffB6tVw6aW+12jIEF9q40/dS9Wq+X0bvv3Wr64bNcrv3zBhgkpmiBQhYknBOTcD/8Ff0P1rnHNzAa1CkiI1bpy7AO6zz/x01rfegpNO8i2If/wDfv55nwc1aeITwYwZfnXdgAG+W0nbuYoUKC7GFMxsmJmlmVna2rVrgw5HAmQGxx/vF8D9+qsvqnr44T4pNG8OJ58ML7+8T2/RCSf4gegXX/SzlTp18s2N1asDex4isSoukoJz7jnnXLJzLrlOnTpBhyPxVZgDAAATKElEQVQxolq13AVwP//sN3L75Rc/SF2/vl8LMXNmqMcoIcEngu++gxtvhFde8SUzHnpIJbpF8oiLpCBSlCZNchfAzZwJ/fr5NRDdu/vP/nvuCU1EqlnTJ4IlS3zf08iRfvHbf/+r8QYRlBSkjDHLXQC3erXvSmrSBG6/3a+cPu003+W0vVFLv61caqpfCHfOOX55tUp0SzkXySmpE4DZwJFmlmFml5nZFWZ2Rej++maWAfwNuC10TM1IxSPlzwEH5C6A++knPyHphx9g0CBf2nvYMPi8+um4BenwxBMwb54vtHf11SrRLeWWuThrMicnJ7s0zR6REsrK8t1LY8fC66/7AemWLf301ovO3kDjZ2/3iyQOPNCPXl9xhd+XVCTOmdk851xyUcep+0jKlQoVchfArV7t/23YEG69FZq0q0XP755i4v0/8Ue7Lrkluj/4IOiwRaJGSUHKrerVcxfAff+9H3f49lu4cGQTGiyYyhU9f+SLjUfiTj8d+vTxB4mUcUoKIuSudfjxR/joI+jd23h5RnO6rHqTo+us48Fp7Vh11Cl+tlKRJVxF4peSgkgeFSrkLoBbvRpeeAFqH3kIN+8axaGZP3HGQyfyWpMb2fHsuCLKt4rEJw00ixTDd9/BuHEw7vldZKypRC1+58LaHzLkviNIHpqEWdARihROA80iYZS9AO7nVZWYnur4v+M3M2ZdbzoNS6JtrRU8cusGVc2QMkFJQWQ/JCTAaacb4z9rxuqVmTx75jvU3LySEffVolHDLLp13s3dd/tSS9oATuKRuo9ESmv5cr694jHGTzuY9zmDeXTAUYFDDs7itNMr0LOnXyzdsGHQgUp5VtzuIyUFkXBZtAhefZW146fzQUYrUiucQWrimfy2sxYAbdtCz57+54QTfHUNkWhRUhAJinMwezZMnIib9BqL1tRjWqU+pNZKYdb6VuzeU4Fq1aBHj9wkccQRaLBaIkpJQSQW7NkDn37qN/t54w22btzNJ9XPJrXpMFI3dea7jGoANG3qk0OvXn5K7IEHBhy3lDlKCiKxZtcuX5V1wgRfqnv7dn6s24XU1n8jNfNUPl5wEFu2GAkJcNxxua2IY4/16ydESkNJQSSWbdsGU6b4BPH++7BrF7ubH8HsrjeSWqUP0+bXZf58f2jt2r7kd/aAdYMGwYYu8UlJQSRebNzoN5yeONHX2MjMhNatWXP2ZXxQZwCp6fWYPh1++80ffswxuV1NXbtqwFqKR0lBJB6tWQOTJ/sWxKxZ/raOHcnql8LCtoNIXVCX1FT47DPYvdtvSXrSSbldTS1basBa8qekIBLvfvnF7yk6YQLMn+8/7bt3h5QUtvS6gE8WH0JqKkyb5jcPAmjePDdBnHyy331UBGIgKZjZGOAsYI1zrk0+9xvwOHAGsB0Y4pybX9R5lRSkXFq2zHcvTZgAS5eGllafBhdeCOecww9ra5Ka6sexP/4Ytm6FihX9gHWvXj5JtG+vAevyLBaSQndgK/ByAUnhDGA4Pil0Bh53znUu6rxKClKuOecXyU2Y4JPE8uV+UOHMM32COPNMdiVUZfZs34JITYUFC/xD69TZe8C6fv1gn4pEV+BJIRREM2BKAUnhWeAT59yE0N/fAj2cc78Wdk4lBZEQ52DOHJ8cJk3yI9HVq8M55/gEcdppkJjIb7/5zeOyWxJr1/qHJyXldjV17QqVKgX7dCSy4iEpTAEecM7NCv39ETDSOVfoJ76Sgkg+MjP9FnITJ8Ibb8CGDXDwwdC3L6Sk+LGIhASysiA9PTdBfPaZX193wAF+wDq7q6lFi6CfkIRbPCSFqcD9+ySFm5xz8/I5dhgwDKBJkybHLl++PGIxi8S97EVyEyf6RXLbtvnFDf36+RZEp045U5S2bIH//Y+cAesff/SnOOywvQesa9QI8PlIWMRDUlD3kUikbdsGU6f6MYj33vMJo3lz33q48EJfpS+P779nrwHrbdv8gHXXrrlJIilJA9bxKB6SwpnA1eQOND/hnOtU1DmVFERKaNMmv0huwoS9FsmRkuJ/9ukz2rXLdy9lJ4n0dH973bp7D1jXqxfAc5H9FnhSMLMJQA+gNvAbcCeQCOCceyY0JfUpoBd+SuolRY0ngJKCSFjkt0guOdm3Hvr1g8aN//SQ1av3HrBet87f3r59bivi+OM1YB2rAk8KkaKkIBJmK1b42UsTJ8K8eX684YQTfOuhb18/l3UfWVl+qmt2gvj8cz9gXb26H4PIThKHHx7A85F8KSmIyP5btswniAkT4JtvchfJpaTAuecWuER682Y/YJ29NuKnn/zthx+emyBOOkkD1kFSUhCRknMOFi/OXST388+5i+RSUuCss6Bq1QIfuu+A9fbtkJi494B1u3YasI4mJQURCQ/n4IsvfIJ47TU/uFC9OvTpk7tIrpCBhJ079x6wXrjQ316vHnTu7MckkpL8v02aqKBfpCgpiEj4ZWbutZNcziK588/3CSK0SK4wv/7qB6w/+MAPYXz7rR+jAKhVyyeI7J/27aFVK9/KkNJRUhCRyNq1C6ZP991Lb7+99yK5lBTfDCjG1/7t231P1YIFftprerov7/THH/7+ypWhTZvcJJGU5PeU0PjE/lFSEJHo2b7d7yQ3caJfJLdzZ+4iuZQUv0huP/qF9uyB777LTRQLFvif9ev9/WZ+WUXeRNG+vYr8FUZJQUSCsWmTbzlMmAAffui7nI4+OncVdQkLKzkHK1fmtiayE0Z2aQ7w4xTZSSI7UbRooQFtUFIQkViwZo0fe5gwAWbO9Lcde6xPDv3757tIbn9t2uQHr/N2Py1Z4nemA1/sr127vVsVbdpAlSqlvnRcUVIQkdiyYkXuTnLzQnUv27b1C+Wyfxo1Csuldu70yyzydj+lp/sCgODHwo86au9EkZTkx8zLKiUFEYld333ny2x88olfDr11q7+9efO9k8QRR4RtjmpWll9Ut2/308qVucc0afLn7qeyMk1WSUFE4sOePb7/Z8YM38U0a1buTkB160K3bn6q6wkn+H6gIqa87q81a/7c/VTQNNnshBGP02SVFEQkPjnnP5Vnzsz9+flnf1+NGr7qXnZLolOniAwObNsGX321d/fTokWwY4e/P3uabN5WRbt2fk1frFJSEJGyIyNj7yTx1Vf+9kqVoGPH3CTRtSsceGBEQtizx5eGytv1lN802bxTZJOSYmearJKCiJRd69f72hnZSWLePP+pbeZXtmV3N51wQkQ/lfNOk82bKLILAsLe02Sz/w1imqySgoiUH9u2+fpM2Uli9my/oA78J3DewevDD4/4yPHGjX6cIu+g9pIlPm9B7jTZvN1PkZ4mq6QgIuXX7t3+k3jmTD+APWsW/P67v69+/b2TRNu2YR+8zs/OnfD113u3KvKbJpu3VdGuXfimySopiIhky8ryCxfyjkusWOHvO/BAP3id3eWUnOxHkqMUVvY02bzdT6tW5R6Td5psr17QpUvJrhUTScHMegGPAwnAC865B/a5vykwBqgD/A4Mcs5lFHZOJQURCYvly/dOEt9842+vXNkX88tuSRx/fNSr761Zk9uSyE4U334Lt90Go0aV7JyBJwUzSwCWAacBGcBc4ELn3Nd5jnkdmOKcG2dmJ+P3aR5c2HmVFEQkItau9YPX2eslFizwdZsqVPBf0/N2OdWtG/Xwtm3zhWlr1SrZ42MhKRwH3OWc6xn6+xYA59z9eY5ZAvR0zmWYmQGbnHP57/cXoqQgIlGxdasfsM5uScyZk7tQ4Ygj9p7h1KxZzC97Lm5SqBjBGBoBK/L8nQF03ueYhcD5+C6mc4EaZnaIc2593oPMbBgwDKBJkyYRC1hEJEf16n5XudNO83/v2uWnvmYnicmT4YUX/H2NGu3dkmjdOm5Ls0aypXABvhUwNPT3YKCTc254nmMaAk8BzYEZ+ATR2jm3qaDzqqUgIjEhK8svoss7LpE9Qlyrll9Il50kjj220C1LoyEWWgoZwKF5/m4MrMp7gHNuFXAegJlVB84vLCGIiMSMChX8QrljjoG//tWvZPvpp72TxJQp/tiqVf20oewk0aVLzNbEiGRLoSJ+oPkUYCV+oHmAc25JnmNqA78757LM7F4g0zl3R2HnVUtBROLGb7/5NRLZg9cLF/oWRkICdOiQmyS6dYPatSMaSuADzaEgzgD+hZ+SOsY5d6+ZjQLSnHPvmFlf4H7A4buP/uqc21nYOZUURCRubd7sS4VntyS+/NKvagO/ci3vuETTpmG9dEwkhUhQUhCRMmPHDkhLy00Sn33mEwf4VWt5k8RRR5VqhpOSgohIvMnMhMWLc7ubZs70XVAAhxwCf/87/O1vJTp1LAw0i4jI/khIyK2Qd801fvD6++9zE0TDhhEPQUlBRCRWmUHLlv7n0kujcsn4XF0hIiIRoaQgIiI5lBRERCSHkoKIiORQUhARkRxKCiIikkNJQUREcigpiIhIjrgrc2Fma4HlJXx4bWBdGMMJl1iNC2I3NsW1fxTX/imLcTV1ztUp6qC4SwqlYWZpxan9EW2xGhfEbmyKa/8orv1TnuNS95GIiORQUhARkRzlLSk8F3QABYjVuCB2Y1Nc+0dx7Z9yG1e5GlMQEZHClbeWgoiIFEJJQUREcpTJpGBmvczsWzP73sxuzuf+ymY2KXT/F2bWLEbiGmJma80sPfQzNEpxjTGzNWb2VQH3m5k9EYp7kZl1iJG4epjZpjyv1x1RiOlQM/ufmX1jZkvM7Np8jon661XMuKL+eoWuW8XMvjSzhaHY/pHPMVF/TxYzrqDekwlmtsDMpuRzX2RfK+dcmfoBEoAfgMOASsBC4Oh9jrkKeCb0ewowKUbiGgI8FcBr1h3oAHxVwP1nAO8DBnQBvoiRuHoAU6L8WjUAOoR+rwEsy+e/Y9Rfr2LGFfXXK3RdA6qHfk8EvgC67HNMEO/J4sQV1Hvyb8Cr+f33ivRrVRZbCp2A751zPzrndgETgT77HNMHGBf6fTJwiplZDMQVCOfcDOD3Qg7pA7zsvDnAQWbWIAbiijrn3K/Oufmh37cA3wCN9jks6q9XMeMKROh12Br6MzH0s+8Ml6i/J4sZV9SZWWPgTOCFAg6J6GtVFpNCI2BFnr8z+PObI+cY59weYBNwSAzEBXB+qMthspkdGuGYiqu4sQfhuFDz/30zax3NC4ea7e3x3zDzCvT1KiQuCOj1CnWHpANrgA+ccwW+ZlF8TxYnLoj+e/JfwE1AVgH3R/S1KotJIb+MuW/2L84x4Vaca74LNHPOHQN8SO63gaAF8XoVx3x8PZd2wJPA29G6sJlVB94ArnPObd737nweEpXXq4i4Anu9nHOZzrkkoDHQycza7HNIIK9ZMeKK6nvSzM4C1jjn5hV2WD63he21KotJIQPIm80bA6sKOsbMKgIHEvluiiLjcs6td87tDP35PHBshGMqruK8plHnnNuc3fx3zr0HJJpZ7Uhf18wS8R+8451zb+ZzSCCvV1FxBfV67RPDRuAToNc+dwXxniwyrgDek12B3mb2M76L+WQze2WfYyL6WpXFpDAXaGlmzc2sEn4g5p19jnkHuDj0e1/gYxcatQkyrn36nXvj+4VjwTvARaFZNV2ATc65X4MOyszqZ/elmlkn/P/P6yN8TQNeBL5xzj1awGFRf72KE1cQr1foWnXM7KDQ71WBU4Gl+xwW9fdkceKK9nvSOXeLc66xc64Z/jPiY+fcoH0Oi+hrVTFcJ4oVzrk9ZnY1kIqf8TPGObfEzEYBac65d/Bvnv+Y2ff4DJsSI3FdY2a9gT2huIZEOi4AM5uAn5lS28wygDvxg244554B3sPPqPke2A5cEiNx9QWuNLM9wB9AShSSe1dgMLA41BcN8HegSZ64gni9ihNXEK8X+JlR48wsAZ+IXnPOTQn6PVnMuAJ5T+4rmq+VylyIiEiOsth9JCIiJaSkICIiOZQUREQkh5KCiIjkUFIQEZEcSgoiUWS+UumfKl+KxAolBRERyaGkIJIPMxsUqrWfbmbPhgqnbTWzf5rZfDP7yMzqhI5NMrM5oaJpb5lZrdDtLczsw1ABuvlmdnjo9NVDxdWWmtn4KFToFSk2JQWRfZjZUUB/oGuoWFomMBA4AJjvnOsAfIpfYQ3wMjAyVDRtcZ7bxwOjQwXojgeyS120B64Djsbvr9E14k9KpJjKXJkLkTA4BV/4bG7oS3xVfGnlLGBS6JhXgDfN7EDgIOfcp6HbxwGvm1kNoJFz7i0A59wOgND5vnTOZYT+TgeaAbMi/7REiqakIPJnBoxzzt2y141mt+9zXGE1YgrrEtqZ5/dM9D6UGKLuI5E/+wjoa2Z1AczsYDNrin+/9A0dMwCY5ZzbBGwwsxNCtw8GPg3tZZBhZueEzlHZzKpF9VmIlIC+oYjswzn3tZndBkw3swrAbuCvwDagtZnNw+921T/0kIuBZ0If+j+SWxV1MPBsqMLlbuCCKD4NkRJRlVSRYjKzrc656kHHIRJJ6j4SEZEcaimIiEgOtRRERCSHkoKIiORQUhARkRxKCiIikkNJQUREcvw/JKzMNqqhQXkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題8】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
